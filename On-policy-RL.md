Reinforcement Learning for Model Fine-tuning
===============================
In this document we provide a brief introduction to reinforcement learning (RL). In the following we first descibe the problem setup and the challenges while solving, then we will will briefly describe the policy gradient style algorithms and why such algorithms are preferred for solving RL in the context of large language models (LLM), and finally we provide experimental results on classical cartpole environment and fine-tuning GPT-2 using RLHF.
# Introduction



# On- vs Off-Policy: Key Take-aways

## Why On-policy?

**Off-policy (Q-learning, DQN, SAC)**

âœ… More sample-efficient (reuse replay buffer).

âš ï¸ Can be harder to stabilize (distribution shift, divergence).

âš–ï¸ Not inherently more â€œcomputationally expensiveâ€ â€” often cheaper per gradient step.

**On-policy (REINFORCE, A2C, PPO)**

âœ… Simple & stable (fresh data, unbiased gradients).

âŒ Less sample-efficient (must discard old trajectories).

ğŸš€ Easier to scale to large policies (why PPO is used in RLHF).

## Algorithm lineage:

- REINFORCE â†’ simplest on-policy policy gradient; high variance.
- Actorâ€“Critic â†’ adds a value function to reduce variance & improve learning.
- PPO / TRPO â†’ clip or constrain updates for stability at scale.

# Reading list for basic understanding

- ğŸ“– Sutton & Barto (2018, 2nd ed.) â€” Ch. 13: Policy Gradient Methods
Foundations: REINFORCE, actorâ€“critic, compatible function approximation, natural gradients.
- ğŸ“„ Mnih et al., 2016 â€” â€œAsynchronous Methods for Deep Reinforcement Learning (A3C)â€
Practical actorâ€“critic with entropy regularization and distributed rollouts.
- ğŸ“„ Schulman et al., 2017 â€” â€œProximal Policy Optimization Algorithms (PPO)â€
Stable, scalable on-policy method; the backbone of RLHF training.
- ğŸ“„ Christiano et al., 2017 â€” â€œDeep Reinforcement Learning from Human Preferencesâ€
Reward modeling from human comparisons; first demonstration of RL with human preference signals.
- ğŸ“„ Ziegler et al., 2019 â€” â€œFine-Tuning Language Models from Human Preferencesâ€
Full RLHF pipeline on language models (SFT â†’ reward model â†’ PPO fine-tuning).