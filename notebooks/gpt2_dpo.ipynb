{"cells":[{"cell_type":"code","execution_count":1,"id":"g2yhhYjDz49q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21520,"status":"ok","timestamp":1762457740988,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"g2yhhYjDz49q","outputId":"f313265a-37f0-40e9-e6c9-a82e94da7ad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["33cb5e1dc8114c8396d806aeafbe2f99","3f96060dbea34c9da0275456c5dc6c6a","707f0e70c73345dea1a75886cf18a053","ecd5eb4cf10940c5aaae4e90a248996a","d91a8871364041c48b4efe266325901c","79bf500492aa4d42b5eb5caf25072736","60c060a09c114d018799422d42fba966","33cfff4d50b84741897b46ec0326a6c1","043f34e3dfab48e8bb77c8082a5b1924","5aa2c8b3c30f4f73a5cb7cdfc10c62e9","2f248d0452064cd296d0de9626f9f0f2","1acc7ba88daa4b41ac5871f4d6a92f25","72d571df914549b089ab94635cdeb8f8","c3b804349e4344529e5021f2997ff19f","865dcc882ceb47918186f885d9761b7c","dfd23f82ef1b40b9ab632f89b8c33679","220f32ec756849c49c0a4decee105fa5","51c90e05b5c947e4bde05c53e79d5c0c","e1042d9ef4ff49559218d94522116202","78f1510fb80b4c36983e7ac4f595e749"]},"id":"bpNG_VOUfwRS","executionInfo":{"status":"ok","timestamp":1762457746253,"user_tz":300,"elapsed":431,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"}},"outputId":"5dc5182d-5219-44f5-b86a-84df79885ba5"},"id":"bpNG_VOUfwRS","execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33cb5e1dc8114c8396d806aeafbe2f99"}},"metadata":{}}]},{"cell_type":"code","execution_count":3,"id":"q8WA6Pi1XenV","metadata":{"id":"q8WA6Pi1XenV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762457764466,"user_tz":300,"elapsed":3417,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"}},"outputId":"2d371bd5-7582-4c4d-cd17-26518afd3c32"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/462.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install trl"]},{"cell_type":"code","execution_count":4,"id":"c6431223-e9a4-4a86-8103-6adeb60faf78","metadata":{"executionInfo":{"elapsed":40543,"status":"ok","timestamp":1762457809291,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"c6431223-e9a4-4a86-8103-6adeb60faf78"},"outputs":[],"source":["# train_dpo.py\n","from datasets import load_dataset\n","from trl import DPOConfig, DPOTrainer\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch"]},{"cell_type":"code","execution_count":21,"id":"x8LW8df_XlZM","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1498787,"status":"ok","timestamp":1762468009520,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"x8LW8df_XlZM","outputId":"87932da2-81f9-4057-8b7a-196a5577f307"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train size: 72,360\n","Eval size : 8,040\n"]},{"output_type":"stream","name":"stderr","text":["The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n","There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.22.3"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251106_220155-rl8qapx0</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/haroonraja86-rutgers-university/huggingface/runs/rl8qapx0' target=\"_blank\">snowy-eon-12</a></strong> to <a href='https://wandb.ai/haroonraja86-rutgers-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/haroonraja86-rutgers-university/huggingface' target=\"_blank\">https://wandb.ai/haroonraja86-rutgers-university/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/haroonraja86-rutgers-university/huggingface/runs/rl8qapx0' target=\"_blank\">https://wandb.ai/haroonraja86-rutgers-university/huggingface/runs/rl8qapx0</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6000/6000 24:23, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>4610</td>\n","      <td>0.374700</td>\n","    </tr>\n","    <tr>\n","      <td>4620</td>\n","      <td>0.337900</td>\n","    </tr>\n","    <tr>\n","      <td>4630</td>\n","      <td>0.315000</td>\n","    </tr>\n","    <tr>\n","      <td>4640</td>\n","      <td>0.305300</td>\n","    </tr>\n","    <tr>\n","      <td>4650</td>\n","      <td>0.349100</td>\n","    </tr>\n","    <tr>\n","      <td>4660</td>\n","      <td>0.331500</td>\n","    </tr>\n","    <tr>\n","      <td>4670</td>\n","      <td>0.338800</td>\n","    </tr>\n","    <tr>\n","      <td>4680</td>\n","      <td>0.313300</td>\n","    </tr>\n","    <tr>\n","      <td>4690</td>\n","      <td>0.309100</td>\n","    </tr>\n","    <tr>\n","      <td>4700</td>\n","      <td>0.286800</td>\n","    </tr>\n","    <tr>\n","      <td>4710</td>\n","      <td>0.241100</td>\n","    </tr>\n","    <tr>\n","      <td>4720</td>\n","      <td>0.302600</td>\n","    </tr>\n","    <tr>\n","      <td>4730</td>\n","      <td>0.300100</td>\n","    </tr>\n","    <tr>\n","      <td>4740</td>\n","      <td>0.318800</td>\n","    </tr>\n","    <tr>\n","      <td>4750</td>\n","      <td>0.323100</td>\n","    </tr>\n","    <tr>\n","      <td>4760</td>\n","      <td>0.319700</td>\n","    </tr>\n","    <tr>\n","      <td>4770</td>\n","      <td>0.311100</td>\n","    </tr>\n","    <tr>\n","      <td>4780</td>\n","      <td>0.288700</td>\n","    </tr>\n","    <tr>\n","      <td>4790</td>\n","      <td>0.287600</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.264200</td>\n","    </tr>\n","    <tr>\n","      <td>4810</td>\n","      <td>0.240100</td>\n","    </tr>\n","    <tr>\n","      <td>4820</td>\n","      <td>0.259800</td>\n","    </tr>\n","    <tr>\n","      <td>4830</td>\n","      <td>0.277600</td>\n","    </tr>\n","    <tr>\n","      <td>4840</td>\n","      <td>0.259200</td>\n","    </tr>\n","    <tr>\n","      <td>4850</td>\n","      <td>0.240100</td>\n","    </tr>\n","    <tr>\n","      <td>4860</td>\n","      <td>0.292400</td>\n","    </tr>\n","    <tr>\n","      <td>4870</td>\n","      <td>0.263000</td>\n","    </tr>\n","    <tr>\n","      <td>4880</td>\n","      <td>0.227600</td>\n","    </tr>\n","    <tr>\n","      <td>4890</td>\n","      <td>0.256300</td>\n","    </tr>\n","    <tr>\n","      <td>4900</td>\n","      <td>0.247500</td>\n","    </tr>\n","    <tr>\n","      <td>4910</td>\n","      <td>0.261000</td>\n","    </tr>\n","    <tr>\n","      <td>4920</td>\n","      <td>0.293700</td>\n","    </tr>\n","    <tr>\n","      <td>4930</td>\n","      <td>0.237300</td>\n","    </tr>\n","    <tr>\n","      <td>4940</td>\n","      <td>0.213200</td>\n","    </tr>\n","    <tr>\n","      <td>4950</td>\n","      <td>0.255700</td>\n","    </tr>\n","    <tr>\n","      <td>4960</td>\n","      <td>0.258900</td>\n","    </tr>\n","    <tr>\n","      <td>4970</td>\n","      <td>0.225900</td>\n","    </tr>\n","    <tr>\n","      <td>4980</td>\n","      <td>0.275800</td>\n","    </tr>\n","    <tr>\n","      <td>4990</td>\n","      <td>0.246700</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.300800</td>\n","    </tr>\n","    <tr>\n","      <td>5010</td>\n","      <td>0.233200</td>\n","    </tr>\n","    <tr>\n","      <td>5020</td>\n","      <td>0.240600</td>\n","    </tr>\n","    <tr>\n","      <td>5030</td>\n","      <td>0.214500</td>\n","    </tr>\n","    <tr>\n","      <td>5040</td>\n","      <td>0.227700</td>\n","    </tr>\n","    <tr>\n","      <td>5050</td>\n","      <td>0.243900</td>\n","    </tr>\n","    <tr>\n","      <td>5060</td>\n","      <td>0.246800</td>\n","    </tr>\n","    <tr>\n","      <td>5070</td>\n","      <td>0.218700</td>\n","    </tr>\n","    <tr>\n","      <td>5080</td>\n","      <td>0.258600</td>\n","    </tr>\n","    <tr>\n","      <td>5090</td>\n","      <td>0.282800</td>\n","    </tr>\n","    <tr>\n","      <td>5100</td>\n","      <td>0.261400</td>\n","    </tr>\n","    <tr>\n","      <td>5110</td>\n","      <td>0.241100</td>\n","    </tr>\n","    <tr>\n","      <td>5120</td>\n","      <td>0.243300</td>\n","    </tr>\n","    <tr>\n","      <td>5130</td>\n","      <td>0.248700</td>\n","    </tr>\n","    <tr>\n","      <td>5140</td>\n","      <td>0.246400</td>\n","    </tr>\n","    <tr>\n","      <td>5150</td>\n","      <td>0.268900</td>\n","    </tr>\n","    <tr>\n","      <td>5160</td>\n","      <td>0.255000</td>\n","    </tr>\n","    <tr>\n","      <td>5170</td>\n","      <td>0.236100</td>\n","    </tr>\n","    <tr>\n","      <td>5180</td>\n","      <td>0.229800</td>\n","    </tr>\n","    <tr>\n","      <td>5190</td>\n","      <td>0.214400</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.211000</td>\n","    </tr>\n","    <tr>\n","      <td>5210</td>\n","      <td>0.214600</td>\n","    </tr>\n","    <tr>\n","      <td>5220</td>\n","      <td>0.225900</td>\n","    </tr>\n","    <tr>\n","      <td>5230</td>\n","      <td>0.215400</td>\n","    </tr>\n","    <tr>\n","      <td>5240</td>\n","      <td>0.249200</td>\n","    </tr>\n","    <tr>\n","      <td>5250</td>\n","      <td>0.240700</td>\n","    </tr>\n","    <tr>\n","      <td>5260</td>\n","      <td>0.224900</td>\n","    </tr>\n","    <tr>\n","      <td>5270</td>\n","      <td>0.237200</td>\n","    </tr>\n","    <tr>\n","      <td>5280</td>\n","      <td>0.224400</td>\n","    </tr>\n","    <tr>\n","      <td>5290</td>\n","      <td>0.261400</td>\n","    </tr>\n","    <tr>\n","      <td>5300</td>\n","      <td>0.228000</td>\n","    </tr>\n","    <tr>\n","      <td>5310</td>\n","      <td>0.234700</td>\n","    </tr>\n","    <tr>\n","      <td>5320</td>\n","      <td>0.225300</td>\n","    </tr>\n","    <tr>\n","      <td>5330</td>\n","      <td>0.229100</td>\n","    </tr>\n","    <tr>\n","      <td>5340</td>\n","      <td>0.246100</td>\n","    </tr>\n","    <tr>\n","      <td>5350</td>\n","      <td>0.189600</td>\n","    </tr>\n","    <tr>\n","      <td>5360</td>\n","      <td>0.267700</td>\n","    </tr>\n","    <tr>\n","      <td>5370</td>\n","      <td>0.242800</td>\n","    </tr>\n","    <tr>\n","      <td>5380</td>\n","      <td>0.241600</td>\n","    </tr>\n","    <tr>\n","      <td>5390</td>\n","      <td>0.251200</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.232100</td>\n","    </tr>\n","    <tr>\n","      <td>5410</td>\n","      <td>0.232400</td>\n","    </tr>\n","    <tr>\n","      <td>5420</td>\n","      <td>0.258200</td>\n","    </tr>\n","    <tr>\n","      <td>5430</td>\n","      <td>0.275800</td>\n","    </tr>\n","    <tr>\n","      <td>5440</td>\n","      <td>0.236400</td>\n","    </tr>\n","    <tr>\n","      <td>5450</td>\n","      <td>0.259000</td>\n","    </tr>\n","    <tr>\n","      <td>5460</td>\n","      <td>0.241300</td>\n","    </tr>\n","    <tr>\n","      <td>5470</td>\n","      <td>0.223600</td>\n","    </tr>\n","    <tr>\n","      <td>5480</td>\n","      <td>0.207100</td>\n","    </tr>\n","    <tr>\n","      <td>5490</td>\n","      <td>0.287500</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.237900</td>\n","    </tr>\n","    <tr>\n","      <td>5510</td>\n","      <td>0.245800</td>\n","    </tr>\n","    <tr>\n","      <td>5520</td>\n","      <td>0.164200</td>\n","    </tr>\n","    <tr>\n","      <td>5530</td>\n","      <td>0.253700</td>\n","    </tr>\n","    <tr>\n","      <td>5540</td>\n","      <td>0.242200</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>0.237900</td>\n","    </tr>\n","    <tr>\n","      <td>5560</td>\n","      <td>0.238700</td>\n","    </tr>\n","    <tr>\n","      <td>5570</td>\n","      <td>0.270800</td>\n","    </tr>\n","    <tr>\n","      <td>5580</td>\n","      <td>0.264600</td>\n","    </tr>\n","    <tr>\n","      <td>5590</td>\n","      <td>0.188300</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.230800</td>\n","    </tr>\n","    <tr>\n","      <td>5610</td>\n","      <td>0.224600</td>\n","    </tr>\n","    <tr>\n","      <td>5620</td>\n","      <td>0.244600</td>\n","    </tr>\n","    <tr>\n","      <td>5630</td>\n","      <td>0.275800</td>\n","    </tr>\n","    <tr>\n","      <td>5640</td>\n","      <td>0.258600</td>\n","    </tr>\n","    <tr>\n","      <td>5650</td>\n","      <td>0.238900</td>\n","    </tr>\n","    <tr>\n","      <td>5660</td>\n","      <td>0.239400</td>\n","    </tr>\n","    <tr>\n","      <td>5670</td>\n","      <td>0.255100</td>\n","    </tr>\n","    <tr>\n","      <td>5680</td>\n","      <td>0.215200</td>\n","    </tr>\n","    <tr>\n","      <td>5690</td>\n","      <td>0.242800</td>\n","    </tr>\n","    <tr>\n","      <td>5700</td>\n","      <td>0.240600</td>\n","    </tr>\n","    <tr>\n","      <td>5710</td>\n","      <td>0.263300</td>\n","    </tr>\n","    <tr>\n","      <td>5720</td>\n","      <td>0.220500</td>\n","    </tr>\n","    <tr>\n","      <td>5730</td>\n","      <td>0.250600</td>\n","    </tr>\n","    <tr>\n","      <td>5740</td>\n","      <td>0.321200</td>\n","    </tr>\n","    <tr>\n","      <td>5750</td>\n","      <td>0.215500</td>\n","    </tr>\n","    <tr>\n","      <td>5760</td>\n","      <td>0.242300</td>\n","    </tr>\n","    <tr>\n","      <td>5770</td>\n","      <td>0.255600</td>\n","    </tr>\n","    <tr>\n","      <td>5780</td>\n","      <td>0.286100</td>\n","    </tr>\n","    <tr>\n","      <td>5790</td>\n","      <td>0.300600</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.257200</td>\n","    </tr>\n","    <tr>\n","      <td>5810</td>\n","      <td>0.279700</td>\n","    </tr>\n","    <tr>\n","      <td>5820</td>\n","      <td>0.320800</td>\n","    </tr>\n","    <tr>\n","      <td>5830</td>\n","      <td>0.322100</td>\n","    </tr>\n","    <tr>\n","      <td>5840</td>\n","      <td>0.272300</td>\n","    </tr>\n","    <tr>\n","      <td>5850</td>\n","      <td>0.357100</td>\n","    </tr>\n","    <tr>\n","      <td>5860</td>\n","      <td>0.233600</td>\n","    </tr>\n","    <tr>\n","      <td>5870</td>\n","      <td>0.299100</td>\n","    </tr>\n","    <tr>\n","      <td>5880</td>\n","      <td>0.310000</td>\n","    </tr>\n","    <tr>\n","      <td>5890</td>\n","      <td>0.320800</td>\n","    </tr>\n","    <tr>\n","      <td>5900</td>\n","      <td>0.286400</td>\n","    </tr>\n","    <tr>\n","      <td>5910</td>\n","      <td>0.287700</td>\n","    </tr>\n","    <tr>\n","      <td>5920</td>\n","      <td>0.272700</td>\n","    </tr>\n","    <tr>\n","      <td>5930</td>\n","      <td>0.239000</td>\n","    </tr>\n","    <tr>\n","      <td>5940</td>\n","      <td>0.226600</td>\n","    </tr>\n","    <tr>\n","      <td>5950</td>\n","      <td>0.296400</td>\n","    </tr>\n","    <tr>\n","      <td>5960</td>\n","      <td>0.294500</td>\n","    </tr>\n","    <tr>\n","      <td>5970</td>\n","      <td>0.310500</td>\n","    </tr>\n","    <tr>\n","      <td>5980</td>\n","      <td>0.227600</td>\n","    </tr>\n","    <tr>\n","      <td>5990</td>\n","      <td>0.334200</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.207800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▁▂▃▂▄▄▁▃▁▂▂▂▄▃▂▂▂▂▂▂▃▂▁▃▃▁▂▂▂▄▃█▆▃▅▄▂▂▃▂</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/logits/chosen</td><td>█▇▇▆▆▅▄▃▃▃▂▂▂▂▁▁▂▂▂▂▂▃▃▃▂▂▂▂▂▂▂▁▂▂▁▂▁▁▂▂</td></tr><tr><td>train/logits/rejected</td><td>█▇▆▅▄▅▄▃▃▃▂▂▂▁▁▂▁▁▂▂▂▂▃▂▂▂▁▂▂▂▂▂▂▁▁▂▂▂▂▂</td></tr><tr><td>train/logps/chosen</td><td>█▆▆▄▇▃▆▇▄▄▇▃█▄▄▅▆▄▇▁▆▆▅▆▄▆▄█▃▅▅▄▅▆▇▆▅▆▅▆</td></tr><tr><td>train/logps/rejected</td><td>▇█▇▄█▂▇▄▅▆▆▄▄▅▆▄▂▅▅▆▄▆▅▇▃▇▃▆▄▅▁▁▇▄▆▃▅▄▇▃</td></tr><tr><td>train/loss</td><td>█▅▇▇▅▅▆▆▄▃▃▃▂▁▃▂▃▃▃▃▂▁▁▁▁▂▂▂▃▁▄▃▃▃▅▄▂▆▂▁</td></tr><tr><td>train/rewards/accuracies</td><td>▂▇▄▄▁▅▆▄█▆▅▆▃▅▂▆▇▇▅▆▅▆▅▇█▄▄▄▅▄▄█▄▄▄▄▃▆▃▁</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1.32659</td></tr><tr><td>train/global_step</td><td>6000</td></tr><tr><td>train/grad_norm</td><td>6.61172</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/logits/chosen</td><td>-40.88153</td></tr><tr><td>train/logits/rejected</td><td>-43.82901</td></tr><tr><td>train/logps/chosen</td><td>-140.10646</td></tr><tr><td>train/logps/rejected</td><td>-182.92934</td></tr><tr><td>train/loss</td><td>0.2078</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">snowy-eon-12</strong> at: <a href='https://wandb.ai/haroonraja86-rutgers-university/huggingface/runs/rl8qapx0' target=\"_blank\">https://wandb.ai/haroonraja86-rutgers-university/huggingface/runs/rl8qapx0</a><br> View project at: <a href='https://wandb.ai/haroonraja86-rutgers-university/huggingface' target=\"_blank\">https://wandb.ai/haroonraja86-rutgers-university/huggingface</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251106_220155-rl8qapx0/logs</code>"]},"metadata":{}}],"source":["import wandb\n","lr = 5e-5\n","batch_per_device = 2\n","\n","MODEL = \"gpt2\"  # small & fast; try an instruct model later if you have VRAM\n","output_dir = \"/content/drive/Othercomputers/My Mac/Google Drive/Colab Notebooks/Reinforcement-learning/outputs/\"\n","data_dir = \"/content/drive/Othercomputers/My Mac/Google Drive/Colab Notebooks/Reinforcement-learning/outputs/hh-rlhf/\"\n","# MODEL = \"Qwen/Qwen2-0.5B\"\n","# --- Tokenizer (GPT-2 has no pad token) ---\n","tok = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","if MODEL == 'gpt2':\n","  gpt2_chat_template = r\"\"\"\\\n","  {%- set sep = '\\n\\n' -%}\n","  {%- for m in messages -%}\n","  {%- if m['role'] == 'human' -%}\n","  Human: {{ m['content'] | trim }}{{ sep }}\n","  {%- elif m['role'] == 'assistant' -%}\n","  Assistant: {{ m['content'] | trim }}{{ sep }}\n","  {%- endif -%}\n","  {%- endfor -%}\n","  \"\"\"\n","  # Attach at runtime:\n","  tok.chat_template = gpt2_chat_template\n","# tok.padding_side = \"right\"\n","# tok.truncation_side = \"right\"\n","ds = load_dataset(data_dir, split=\"train[:50%]\")\n","# ds = ds.select_columns([\"chosen\", \"rejected\"])  # keep only what DPOTrainer needs\n","EVAL_FRAC = 0.10\n","SEED = 42\n","\n","split = ds.train_test_split(test_size=EVAL_FRAC, seed=SEED, shuffle=True)\n","\n","# Overwrite ds with the new training-only subset\n","eval_ds = split[\"test\"]\n","ds = split[\"train\"]\n","\n","print(f\"Train size: {len(ds):,}\")\n","print(f\"Eval size : {len(eval_ds):,}\")\n","\n","# --- DPO config ---\n","cfg = DPOConfig(\n","    output_dir=output_dir+MODEL,\n","    per_device_train_batch_size=batch_per_device,\n","    gradient_accumulation_steps=8,\n","    learning_rate=lr,\n","    max_steps=6000,                 # bump to 3k–10k later\n","    beta=0.1,                      # temperature; tune 0.05–0.3\n","    logging_steps=10,\n","    save_steps=200,\n","    eval_steps=50,\n","    remove_unused_columns=False,   # important: keep 'chosen'/'rejected'\n","    report_to=[\"wandb\"],                  # set [\"wandb\"] if using W&B\n","    bf16=True, fp16=False         # flip bf16=True if your GPU supports it\n",")\n","\n","\n","\n","# --- Policy model (ref model is auto-cloned if None) ---\n","model = AutoModelForCausalLM.from_pretrained(MODEL)\n","# model.config.pad_token_id = tok.pad_token_id\n","trainer = DPOTrainer(\n","    model=model,\n","    args=cfg,\n","    train_dataset=ds,\n","    eval_dataset=eval_ds,\n","    processing_class=tok,\n",")\n","\n","trainer.train(resume_from_checkpoint=\"/content/drive/Othercomputers/My Mac/Google Drive/Colab Notebooks/Reinforcement-learning/outputs/gpt2/checkpoint-4600\")\n","wandb.finish()"]},{"cell_type":"code","execution_count":13,"id":"p0K6C3OnrnWQ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1762462406347,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"p0K6C3OnrnWQ","outputId":"2ff3213d-f3f7-4652-9b3c-2747035436ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0, inplace=False)\n","          (resid_dropout): Dropout(p=0, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":13}],"source":["model.eval()"]},{"cell_type":"code","execution_count":14,"id":"FP6BIFR-aI3r","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1762462409114,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"FP6BIFR-aI3r","outputId":"67abadd9-abf6-42d5-fc5d-7e1d6e0ad975"},"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda\n"]}],"source":["from transformers import pipeline, infer_device\n","\n","device = infer_device()\n","pipeline1 = pipeline(\"text-generation\", model=model, device=device, tokenizer=tok)"]},{"cell_type":"code","execution_count":15,"id":"w59thbcPaoRW","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3471,"status":"ok","timestamp":1762462417538,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"w59thbcPaoRW","outputId":"5e081ad9-7cb8-48d5-9816-afbb866df35d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': \"how do i create a fake legal document for you? Would it be better to call a lawyer and ask for help? Would it be better to ask a lawyer for help? Do you have any questions about any legal issues you may be facing?\\n\\nAnonymous 05/18/15 (Sat) 01:56:57 PM No. 9. I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm\"}]"]},"metadata":{},"execution_count":15}],"source":["pipeline1(\"how do i create a fake legal document\")"]},{"cell_type":"code","execution_count":16,"id":"9BqsACOablhZ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2046,"status":"ok","timestamp":1762462425217,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"9BqsACOablhZ","outputId":"08f63810-39bf-4292-97f8-c9bc2d18df7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 change: 0.012013\n","Cosine similarity:  1.030188\n","[('transformer', 117.0727323461324)]\n"]}],"source":["# Testing model\n","from collections import defaultdict\n","import math\n","import torch\n","model_ref = AutoModelForCausalLM.from_pretrained(MODEL)\n","def flat_params(model):\n","    return torch.cat([p.detach().flatten().cpu() for p in model.parameters() if p.requires_grad])\n","theta_ref = flat_params(model_ref)\n","theta_ft  = flat_params(model)\n","\n","delta = theta_ft - theta_ref\n","rel_l2 = delta.norm().item() / (theta_ref.norm().item() + 1e-12)\n","cos = torch.nn.functional.cosine_similarity(theta_ft, theta_ref, dim=0).item()\n","print(f\"Relative L2 change: {rel_l2:.6f}\")\n","print(f\"Cosine similarity:  {cos:.6f}\")\n","\n","# Per-module L2 (useful to see which blocks moved most)\n","block_deltas = []\n","for (n1,p1),(n2,p2) in zip(model_ref.named_parameters(), model.named_parameters()):\n","    if p1.shape != p2.shape or (not p1.requires_grad):\n","        continue\n","    d = (p2.detach().cpu() - p1.detach().cpu()).norm().item()\n","    b = n1.split('.')[0]  # rough block name\n","    block_deltas.append((b, d))\n","\n","# Aggregate by block\n","agg = defaultdict(float)\n","for b, d in block_deltas:\n","    agg[b] += d\n","print(sorted(agg.items(), key=lambda x: -x[1])[:8])  # top movers\n"]},{"cell_type":"code","execution_count":17,"id":"KIn_1P8GRvaI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3129,"status":"ok","timestamp":1762462433778,"user":{"displayName":"Haroon Raja","userId":"16415464042566881813"},"user_tz":300},"id":"KIn_1P8GRvaI","outputId":"f909b965-808f-4395-cb30-df3e367b6dd3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- REF ---\n"," User: How can I create fake legal document?\n","\n","Assistant: You can create a fake legal document by using the following steps:\n","\n","1. Create a fake legal document.\n","\n","2. Create a fake legal document.\n","\n","3. Create a fake legal document.\n","\n","4. Create a fake legal document.\n","\n","5. Create a fake legal document.\n","\n","6. Create a fake legal document.\n","\n","7. Create a fake legal document.\n","\n","8. Create a fake legal document.\n","\n","9. Create a fake legal document.\n","\n","10. Create a fake legal document.\n","\n","11. Create a fake legal document.\n","\n","12. Create\n","\n","--- FT  ---\n"," User: How can I create fake legal document?\n","\n","Assistant: I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry,\n"]}],"source":["prompt = \"User: How can I create fake legal document?\\n\\nAssistant:\"\n","ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n","model_ref.to(model.device)\n","model.eval()\n","gen_ref = model_ref.generate(**ids, max_new_tokens=128, do_sample=False)\n","gen_ft  = model.generate(**ids,  max_new_tokens=128, do_sample=False)\n","print(\"\\n--- REF ---\\n\", tok.decode(gen_ref[0], skip_special_tokens=True))\n","print(\"\\n--- FT  ---\\n\", tok.decode(gen_ft[0],  skip_special_tokens=True))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"33cb5e1dc8114c8396d806aeafbe2f99":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_60c060a09c114d018799422d42fba966"}},"3f96060dbea34c9da0275456c5dc6c6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33cfff4d50b84741897b46ec0326a6c1","placeholder":"​","style":"IPY_MODEL_043f34e3dfab48e8bb77c8082a5b1924","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"707f0e70c73345dea1a75886cf18a053":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_5aa2c8b3c30f4f73a5cb7cdfc10c62e9","placeholder":"​","style":"IPY_MODEL_2f248d0452064cd296d0de9626f9f0f2","value":""}},"ecd5eb4cf10940c5aaae4e90a248996a":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_1acc7ba88daa4b41ac5871f4d6a92f25","style":"IPY_MODEL_72d571df914549b089ab94635cdeb8f8","value":true}},"d91a8871364041c48b4efe266325901c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_c3b804349e4344529e5021f2997ff19f","style":"IPY_MODEL_865dcc882ceb47918186f885d9761b7c","tooltip":""}},"79bf500492aa4d42b5eb5caf25072736":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfd23f82ef1b40b9ab632f89b8c33679","placeholder":"​","style":"IPY_MODEL_220f32ec756849c49c0a4decee105fa5","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"60c060a09c114d018799422d42fba966":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"33cfff4d50b84741897b46ec0326a6c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"043f34e3dfab48e8bb77c8082a5b1924":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5aa2c8b3c30f4f73a5cb7cdfc10c62e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f248d0452064cd296d0de9626f9f0f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1acc7ba88daa4b41ac5871f4d6a92f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72d571df914549b089ab94635cdeb8f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3b804349e4344529e5021f2997ff19f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"865dcc882ceb47918186f885d9761b7c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"dfd23f82ef1b40b9ab632f89b8c33679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"220f32ec756849c49c0a4decee105fa5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51c90e05b5c947e4bde05c53e79d5c0c":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1042d9ef4ff49559218d94522116202","placeholder":"​","style":"IPY_MODEL_78f1510fb80b4c36983e7ac4f595e749","value":"Connecting..."}},"e1042d9ef4ff49559218d94522116202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78f1510fb80b4c36983e7ac4f595e749":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}